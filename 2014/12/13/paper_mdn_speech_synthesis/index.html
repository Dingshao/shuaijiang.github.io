
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  
    <title>Paper Reading： 《DEEP MIXTURE DENSITY NETWORKS FOR ACOUSTIC MODELING IN STATISTICAL PARAMETRIC SPEECH SYNTHESIS》 | shuaijiang's blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="shuaijiang">
    
    <meta name="description" content="论文信息

作者：Heiga Zen, Andrew Senior
单位：Google
会议：ICASSP
发表日期： 2014
论文链接：https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak13To14/p3872-zen.pdf

简介
利用深层神经网络（DNNs）的参数语音合成（SPSS），被证明具有生成自然的合成语音的能力。然而，语音合成中基于DNN的声学建模还有不足之处，例如目标函数是单峰的、缺少预测方差的能力。为了解决这些局限，本文研究了利用混合密度的输出层，它能够在给定输入特征条件下，估计输出特征的全概率密度函数。实验的客观和主观结果表明利用混合密度输出层提高了预测的声学特征的准确性、以及合成语音的自然度。">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="shuaijiang's blog" title="shuaijiang's blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="shuaijiang's blog">shuaijiang's blog</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:shuaijiang.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2014/12/13/paper_mdn_speech_synthesis/" title="Paper Reading： 《DEEP MIXTURE DENSITY NETWORKS FOR ACOUSTIC MODELING IN STATISTICAL PARAMETRIC SPEECH SYNTHESIS》" itemprop="url">Paper Reading： 《DEEP MIXTURE DENSITY NETWORKS FOR ACOUSTIC MODELING IN STATISTICAL PARAMETRIC SPEECH SYNTHESIS》</a>
  </h1>
  <p class="article-author">By
    
      <a href="http://shuaijiang.github.io" title="shuaijiang">shuaijiang</a>
    </p>
  <p class="article-time">
    <time datetime="2014-12-13T07:36:02.000Z" itemprop="datePublished">12月 13 2014</time>
    更新日期:<time datetime="2014-12-13T13:36:33.000Z" itemprop="dateModified">12月 13 2014</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<h1>论文信息</h1>
<ul>
<li>作者：Heiga Zen, Andrew Senior</li>
<li>单位：Google</li>
<li>会议：ICASSP</li>
<li>发表日期： 2014</li>
<li>论文链接：<a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak13To14/p3872-zen.pdf" title="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak13To14/p3872-zen.pdf" target="_blank"><a href="https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak13To14/p3872-zen.pdf">https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/Speak13To14/p3872-zen.pdf</a></a></li>
</ul>
<h1>简介</h1>
<p>利用深层神经网络（DNNs）的参数语音合成（SPSS），被证明具有生成自然的合成语音的能力。然而，语音合成中基于DNN的声学建模还有不足之处，例如目标函数是单峰的、缺少预测方差的能力。<br>为了解决这些局限，本文研究了利用混合密度的输出层，它能够在给定输入特征条件下，估计输出特征的全概率密度函数。实验的客观和主观结果表明利用混合密度输出层提高了预测的声学特征的准确性、以及合成语音的自然度。<br><a id="more"></a><br>然而，语音合成中基于DNN的声学建模有一些局限，本文解决了DNN的以下两个局限：<br>1. 基于DNN的声学模型，利用均方误差（Mean Squared Error，MSE）作为它的目标函数，不具有对比单高斯分布更复杂分布的建模能力。<br>2. 一个人工神经网络（ANN）输出只提供了均值。统计参数语音合成中所使用的参数生成算法，需要声学特征的均值和方差，在静态和动态特征的约束下，来得到声学特征最大可能的轨迹。</p>
<p>为了解决这些局限，本文研究了利用混合密度函数作为SPSS的声学模型。MSD能够在输入特征的条件下，给出输出特征的概率密度函数。MSD可以是多峰的回归，并且可以预测方差。</p>
<h1>基于MDN的语音合成</h1>
<p>MDN联合了一个混合模型和一个人工神经网络。本文利用了一个基于高斯混合模型的MDN。一个MDN M 将输入特征x映射到GMM的参数，这样就就给出了在输入特征条件下，输出特征y的全概率密度函数。<br><img src="/paper_image/MDN_pdf.png" alt="MDN"></p>
<p><img src="/paper_image/MDN_parameter.png" alt="MDN参数"><br>其中，$z_m^(w)$,$z_m^(\sigma)$,$z_m^(\mu)$是MDN输出层参数，分别是GMM中第m个分量的混合权重、方差、均值。</p>
<p>训练MDN的过程就是在给定数据条件下，最大化M的对数似然：<br><img src="/paper_image/MDN_model.png" alt="MDN的训练目标"></p>
<p>下图是基于深层MDN的语音合成的框图。首先，将输入文本转换成文本特征序列；第二，每个语音单元的时长通过一个时长预测模块得到；然后，在给定文本特征的条件下，利用训练好的DMDN，预测包括谱参数和激励参数以及它们的差分在内的声学特征的GMM；利用预测的GMM序列，使用语音参数生成算法，可以得到平滑的声学特征轨迹；最后，波形生成模块，对给定的声学特征输出合成的语音波形<br><img src="/paper_image/MDN_based_speech_synthesis.png" alt="基于MDN的语音合成"></p>
<h1>实验</h1>
<h2>实验设置</h2>
<ul>
<li>数据：专业女性，英语语音数据，33000句</li>
<li>输入特征：342维二值特征，用于类别上下文信息（例如音子ID、重音标记）；25维的数值特征，用于数值上下文信息（例如词中的音节数目、当前音节在短语中的位置）</li>
<li>输出特征：40维梅尔倒谱、对数基频、5带宽的非周期分量，以及他们的一阶、二阶差分，共3*（40+1+5）=138维</li>
<li>MDN：基于DNN系统的权重，通过最小化输出特征和预测值之间的均方误差来训练得到；基于MDN系统的权重，通过最大化给定训练数据条件下的模型对数似然来得到。</li>
</ul>
<h2>客观评价</h2>
<p><img src="/paper_image/MDN_object_evaluation.png" alt="客观指标"></p>
<h2>主观评价</h2>
<p><img src="/paper_image/MDN_subject_evaluation.png" alt="主观指标"></p>
<p>从表格中可以看到，具有方差对预测倒谱以及非周期分量有帮助，并且可以提高合成语音的自然度。GMM的多个分量，有助于基频的预测、以及提高合成语音的自然度。</p>
<h1>结论</h1>
<p>本文通过混合密度模型（MDNs）扩展了基于DNN的SPSS。基于DMDN的方法可以缓解语音合成中基于DNN的声学建模的局限：缺少方差，目标函数的单峰特性。客观和主观评价表明，具有方差和多个混合分量的混合密度输出层有助于提高预测声学特征的准确性，并且显著提高合成语音的自然度。</p>
<p>下一步工作包括探索更好的网络结构以及训练网络的优化算法。利用参数生成算法的DMDN的评价，也需要考虑到global variance。</p>
  
	</div>
		<footer class="article-footer clearfix">

  <div class="article-tags">
  
  <span></span> <a href="/tags/PaperReading/">PaperReading</a><a href="/tags/语音合成/">语音合成</a>
  </div>




<div class="article-share" id="share">

  <div data-url="http://shuaijiang.github.io/2014/12/13/paper_mdn_speech_synthesis/" data-title="Paper Reading： 《DEEP MIXTURE DENSITY NETWORKS FOR ACOUSTIC MODELING IN STATISTICAL PARAMETRIC SPEECH SYNTHESIS》 | shuaijiang's blog" data-tsina="null" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2014/12/15/paper_rnn_tts/" title="Paper Reading：《TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks》">
  <strong>PREVIOUS:</strong><br/>
  <span>
  Paper Reading：《TTS Synthesis with Bidirectional LSTM based Recurrent Neural Networks》</span>
</a>
</div>


<div class="next">
<a href="/2014/12/10/paper_dnn_speech_synthesis/"  title="Paper Reading:《STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS》">
 <strong>NEXT:</strong><br/> 
 <span>Paper Reading:《STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS》
</span>
</a>
</div>

</nav>

	
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			<li><a href="/tags/PaperReading/" title="PaperReading">PaperReading<sup>4</sup></a></li>
		
			<li><a href="/tags/Perl/" title="Perl">Perl<sup>1</sup></a></li>
		
			<li><a href="/tags/ubuntu/" title="ubuntu">ubuntu<sup>1</sup></a></li>
		
			<li><a href="/tags/信息浪潮/" title="信息浪潮">信息浪潮<sup>1</sup></a></li>
		
			<li><a href="/tags/机器学习/" title="机器学习">机器学习<sup>3</sup></a></li>
		
			<li><a href="/tags/语音合成/" title="语音合成">语音合成<sup>7</sup></a></li>
		
			<li><a href="/tags/语音识别/" title="语音识别">语音识别<sup>1</sup></a></li>
		
			<li><a href="/tags/随笔/" title="随笔">随笔<sup>2</sup></a></li>
		
		</ul>
</div>


  <div class="rsspart">
	<a href="null" target="_blank" title="rss">RSS 订阅</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2015 
		
		<a href="http://shuaijiang.github.io" target="_blank" title="shuaijiang">shuaijiang</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>






  </body>
</html>
